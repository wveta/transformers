{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYpDFySzrk4a"
      },
      "source": [
        "# Seminar on Graphs for NLP: Vector representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlhZKxv4rs2F"
      },
      "source": [
        "## Plan for today:\n",
        "\n",
        "#### 0. What a taxonomy is. Taxonomy Enrichment task.\n",
        "#### 1. Graph Neural networks: GCN and GAT\n",
        "#### 2. GATv2\n",
        "#### 3. GraphBERT: Only Attention is Needed for Learning Graph Representations\n",
        "#### 4. OpenHGNN library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z0GruYny1kG"
      },
      "source": [
        "# 0. Taxonomy\n",
        "\n",
        "A taxonomy is a hierarchical structure of units in terms if class inclusion such that superordinate units in the hierarchy include, or subsume, all items in subordinate units. Taxonomies are typically represented as having tree structures.\n",
        "\n",
        "![](https://www.digital-mr.com/media/cache/51/6f/516f493d37a7b4895f678843b6383e48.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxGSENmE6IsK"
      },
      "source": [
        "Taxonomies can be represented as graphs!\n",
        "\n",
        "Let us download the most popular and well-known taxonomy called WordNet. You may also use the `from nltk.corpus import wordnet as wn`, but keep in mind that you can operate with earlier versions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi-hshM6rL9z",
        "outputId": "11174399-c664-4ae3-cd2e-aabef2be6eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C91pcHIwsg_v",
        "outputId": "3c9aff91-8661-4936-cabf-139f4f4b2c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (23.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "V5VOiYm_2jjR",
        "outputId": "e3d5ff39-76e7-4f8c-e870-0569717f0db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o 'wordnet_n_is_directed_12_en_synsets.zip' 'https://drive.google.com/u/0/uc?id=1TvWsvz8UC0RPKHBx2GRi-iChVG4oTz-m&export=download&confirm=t'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_tZOA9BnlR0",
        "outputId": "a36b7a17-8679-46cc-e7b9-e7b32f402a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  206M  100  206M    0     0  81.5M      0  0:00:02  0:00:02 --:--:--  114M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip wordnet_n_is_directed_12_en_synsets.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORTr2AWjwIvu",
        "outputId": "e50a0036-57ba-4585-c5d4-735fa3a7f618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wordnet_n_is_directed_12_en_synsets.zip\n",
            "   creating: wordnet_n_is_directed_1_en_synsets/\n",
            "  inflating: wordnet_n_is_directed_1_en_synsets/link  \n",
            "   creating: wordnet_n_is_directed_1_en_synsets/.ipynb_checkpoints/\n",
            "  inflating: wordnet_n_is_directed_1_en_synsets/.ipynb_checkpoints/link-checkpoint  \n",
            "  inflating: wordnet_n_is_directed_1_en_synsets/node  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTwSPeZcjmZT",
        "outputId": "7655cd2b-a7e5-4a37-f064-874c2b152a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.poincare import PoincareModel\n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "kX-L6-NLeKJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "id": "Ap6gnMUfy146"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset(\"dog.n.01\").hyponyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40OqktlGy5Wd",
        "outputId": "4e5cdbf0-2b60-44df-d064-da43e3425ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('basenji.n.01'),\n",
              " Synset('corgi.n.01'),\n",
              " Synset('cur.n.01'),\n",
              " Synset('dalmatian.n.02'),\n",
              " Synset('great_pyrenees.n.01'),\n",
              " Synset('griffon.n.02'),\n",
              " Synset('hunting_dog.n.01'),\n",
              " Synset('lapdog.n.01'),\n",
              " Synset('leonberg.n.01'),\n",
              " Synset('mexican_hairless.n.01'),\n",
              " Synset('newfoundland.n.01'),\n",
              " Synset('pooch.n.01'),\n",
              " Synset('poodle.n.01'),\n",
              " Synset('pug.n.01'),\n",
              " Synset('puppy.n.01'),\n",
              " Synset('spitz.n.01'),\n",
              " Synset('toy_dog.n.01'),\n",
              " Synset('working_dog.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = f\"wordnet_n_is_directed_1_en_synsets/\"\n",
        "\n",
        "link_path = os.path.join(path, \"link\")\n",
        "node_path = os.path.join(path, \"node\")"
      ],
      "metadata": {
        "id": "XeGdLaEqekbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2synset = {}\n",
        "fasttext_dict = {}\n",
        "\n",
        "with open(node_path) as f:\n",
        "    for line in f:\n",
        "        line_split = line.split(\"\\t\")\n",
        "        id2synset[line_split[0].strip()] = line_split[-1].strip()\n",
        "        fasttext_dict[line_split[-1].strip()] = np.array([float(num) for num in line_split[1:-1]])"
      ],
      "metadata": {
        "id": "C26HaOfGfDkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link_pairs = set()\n",
        "with open(link_path) as f:\n",
        "    for line in f:\n",
        "        line_split = line.split(\"\\t\")\n",
        "        link_pairs.add((id2synset[line_split[0].strip()], id2synset[line_split[-1].strip()]))"
      ],
      "metadata": {
        "id": "i7hmkbEXfFIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Arh9U60uDH8"
      },
      "source": [
        "# 4. Graph Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "46UD5px_rcNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils"
      ],
      "metadata": {
        "id": "B1qcGlrbre_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM2IXSZMuIq8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.utils import train_test_split_edges\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPMU0mL6IJEA"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NAxJKWLmkae"
      },
      "outputs": [],
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "fasttext = KeyedVectors(vector_size=300)\n",
        "fasttext.add_vectors(list(fasttext_dict.keys()), list(fasttext_dict.values()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx"
      ],
      "metadata": {
        "id": "JL1yQgaD4mio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.DiGraph()\n",
        "\n",
        "for pair in link_pairs:\n",
        "    G.add_edge(*pair)"
      ],
      "metadata": {
        "id": "WkLOHE1t4qJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in link_pairs:\n",
        "    print(pair)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNhTMIM9qPsQ",
        "outputId": "03cc6c4f-2306-4661-dd9c-d14979baa172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('criterion.n.02', 'design_criteria.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgC4chwuIJEC"
      },
      "outputs": [],
      "source": [
        "def create_edge_list(G):\n",
        "    starts = []\n",
        "    ends = []\n",
        "    for left, right in G.edges:\n",
        "        if left in fasttext.key_to_index and right in fasttext.key_to_index:\n",
        "            starts.append(fasttext.key_to_index[left])\n",
        "            ends.append(fasttext.key_to_index[right])\n",
        "    return torch.tensor([starts, ends], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILgb5IF4a2A9"
      },
      "outputs": [],
      "source": [
        "index_to_key = dict(map(reversed, fasttext.key_to_index.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_iX7WH2nCSv"
      },
      "outputs": [],
      "source": [
        "edge_index = create_edge_list(G)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQhXglBvqwCM",
        "outputId": "3c4bcc29-d639-4c3e-8d70-3a69ec1603f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 72370])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([fasttext[index_to_key[int(i)]] for i in index_to_key], dtype=torch.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZJdPogQy1kG",
        "outputId": "15c3106b-f45f-4a11-ff42-e7ba5f68c7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-ceafeacad9cc>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  x = torch.tensor([fasttext[index_to_key[int(i)]] for i in index_to_key], dtype=torch.float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poQi6NZorQfp",
        "outputId": "84ffbcf8-5110-4818-826f-ca878c07508c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([78748, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = Data(x=x, edge_index=edge_index)\n",
        "#data = train_test_split_edges(data)"
      ],
      "metadata": {
        "id": "WC3VgJ0FyiiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import RandomLinkSplit"
      ],
      "metadata": {
        "id": "TCVO4jfK1LFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = RandomLinkSplit(is_undirected=True, split_labels=True)\n",
        "train_data, val_data, test_data = transform(data)"
      ],
      "metadata": {
        "id": "lPA8jPvZ1IIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCZE_0e0IJED"
      },
      "source": [
        "### GCN and GAT Encoder\n",
        "\n",
        "The following code snippet describes the Encoder module with GCN or GAT networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-naFqNRumvk"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mode=\"gcn\"):\n",
        "        super(Encoder, self).__init__()\n",
        "        if mode == \"gcn\":\n",
        "            self.conv1 = pyg_nn.GCNConv(in_channels, 2 * out_channels, cached=True)\n",
        "            self.conv2 = pyg_nn.GCNConv(2 * out_channels, out_channels, cached=True)\n",
        "        elif mode == 'gat':\n",
        "            self.conv1 = pyg_nn.GATConv(in_channels, 2 * out_channels)\n",
        "            self.conv2 = pyg_nn.GATConv(2 * out_channels, out_channels)\n",
        "        else:\n",
        "            raise Exception(\"Encoder mode is not recognized, try gcn/gat\")\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(x, train_pos_edge_index)\n",
        "    loss = model.recon_loss(z, train_pos_edge_index)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    writer.add_scalar(\"loss\", loss.item(), epoch)\n",
        "    return loss.item()\n",
        "\n",
        "def test(pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(x, train_pos_edge_index)\n",
        "    return model.test(z, pos_edge_index, neg_edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eplyGdMwvF57",
        "outputId": "99665dec-4b0c-4bda-fca6-6d79365d6ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA availability: True\n"
          ]
        }
      ],
      "source": [
        "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "channels = 64\n",
        "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('CUDA availability:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHEHYEbcXCFG"
      },
      "source": [
        "## Variational Graph Auto-Encoders\n",
        "\n",
        "https://arxiv.org/pdf/1611.07308.pdf\n",
        "\n",
        "The pipeline is working as follows: first, we train a graph autoencoder with GCN or GAT under the hoot. During the evaluation phase, the latent representations of the autoencoder are actually the embeddings we are looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqZGTsQ7vRQM",
        "outputId": "395b9e73-ef40-451a-d222-f10cd7a86cc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, AUC: 0.8323, AP: 0.8188, Loss: 0.9427\n",
            "Epoch: 020, AUC: 0.8602, AP: 0.8515, Loss: 0.8738\n",
            "Epoch: 030, AUC: 0.8825, AP: 0.8768, Loss: 0.8399\n",
            "Epoch: 040, AUC: 0.8897, AP: 0.8882, Loss: 0.8183\n",
            "Epoch: 050, AUC: 0.8939, AP: 0.8942, Loss: 0.8054\n",
            "Epoch: 060, AUC: 0.8984, AP: 0.8996, Loss: 0.8016\n",
            "Epoch: 070, AUC: 0.9007, AP: 0.9029, Loss: 0.7923\n",
            "Epoch: 080, AUC: 0.9032, AP: 0.9061, Loss: 0.7850\n",
            "Epoch: 090, AUC: 0.9072, AP: 0.9100, Loss: 0.7864\n",
            "Epoch: 100, AUC: 0.9097, AP: 0.9124, Loss: 0.7830\n",
            "Epoch: 110, AUC: 0.9108, AP: 0.9135, Loss: 0.7811\n",
            "Epoch: 120, AUC: 0.9120, AP: 0.9150, Loss: 0.7746\n",
            "Epoch: 130, AUC: 0.9132, AP: 0.9163, Loss: 0.7733\n",
            "Epoch: 140, AUC: 0.9142, AP: 0.9173, Loss: 0.7732\n",
            "Epoch: 150, AUC: 0.9154, AP: 0.9184, Loss: 0.7723\n",
            "Epoch: 160, AUC: 0.9154, AP: 0.9189, Loss: 0.7705\n",
            "Epoch: 170, AUC: 0.9168, AP: 0.9204, Loss: 0.7668\n",
            "Epoch: 180, AUC: 0.9170, AP: 0.9208, Loss: 0.7693\n",
            "Epoch: 190, AUC: 0.9163, AP: 0.9206, Loss: 0.7653\n",
            "Epoch: 200, AUC: 0.9174, AP: 0.9216, Loss: 0.7621\n",
            "Epoch: 210, AUC: 0.9167, AP: 0.9212, Loss: 0.7634\n",
            "Epoch: 220, AUC: 0.9171, AP: 0.9214, Loss: 0.7610\n",
            "Epoch: 230, AUC: 0.9176, AP: 0.9221, Loss: 0.7592\n",
            "Epoch: 240, AUC: 0.9180, AP: 0.9225, Loss: 0.7602\n",
            "Epoch: 250, AUC: 0.9172, AP: 0.9223, Loss: 0.7611\n",
            "Epoch: 260, AUC: 0.9180, AP: 0.9229, Loss: 0.7616\n",
            "Epoch: 270, AUC: 0.9172, AP: 0.9223, Loss: 0.7574\n",
            "Epoch: 280, AUC: 0.9176, AP: 0.9225, Loss: 0.7588\n",
            "Epoch: 290, AUC: 0.9180, AP: 0.9231, Loss: 0.7577\n",
            "Epoch: 300, AUC: 0.9183, AP: 0.9230, Loss: 0.7590\n",
            "Epoch: 310, AUC: 0.9193, AP: 0.9231, Loss: 0.7541\n",
            "Epoch: 320, AUC: 0.9185, AP: 0.9231, Loss: 0.7549\n",
            "Epoch: 330, AUC: 0.9179, AP: 0.9230, Loss: 0.7536\n",
            "Epoch: 340, AUC: 0.9182, AP: 0.9231, Loss: 0.7535\n",
            "Epoch: 350, AUC: 0.9184, AP: 0.9235, Loss: 0.7522\n",
            "Epoch: 360, AUC: 0.9175, AP: 0.9233, Loss: 0.7508\n",
            "Epoch: 370, AUC: 0.9175, AP: 0.9231, Loss: 0.7502\n",
            "Epoch: 380, AUC: 0.9172, AP: 0.9229, Loss: 0.7530\n",
            "Epoch: 390, AUC: 0.9181, AP: 0.9233, Loss: 0.7517\n",
            "Epoch: 400, AUC: 0.9174, AP: 0.9230, Loss: 0.7519\n"
          ]
        }
      ],
      "source": [
        "model = pyg_nn.GAE(Encoder(300, channels, 'gcn')).to(dev)\n",
        "x, train_pos_edge_index = train_data.x.to(dev), train_data.pos_edge_label_index.to(dev)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1, 401):\n",
        "    loss = train(epoch)\n",
        "    auc, ap = test(test_data.pos_edge_label_index, test_data.neg_edge_label_index)\n",
        "    writer.add_scalar(\"AUC\", auc, epoch)\n",
        "    writer.add_scalar(\"AP\", ap, epoch)\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}, Loss: {:.4f}'.format(epoch, auc, ap, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3__lKOCOXNKj"
      },
      "source": [
        "#### Examples\n",
        "\n",
        "Let us see the nearest neighbours for the unseen words from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mWcc2HlvVLf"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "new_x = torch.tensor([fasttext[index_to_key[i]] for i in index_to_key], dtype=torch.float).to(dev)\n",
        "z = model.encode(new_x, train_pos_edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2syns = {}\n",
        "syns2id = {}\n",
        "with open('wordnet_n_is_directed_1_en_synsets/node') as f:\n",
        "    for line in f:\n",
        "        id2syns[line.split()[0]] = line.split()[-1]\n",
        "        syns2id[line.split()[-1]] = line.split()[0]"
      ],
      "metadata": {
        "id": "HoDN-uTq8bem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "par2orph = {}\n",
        "orph2par = {}\n",
        "with open('wordnet_n_is_directed_1_en_synsets/link') as f:\n",
        "    for line in f:\n",
        "        par_id = line.split()[0]\n",
        "        child_id = line.split()[-1]\n",
        "\n",
        "        if \"ORPHAN_\" in id2syns[child_id]:\n",
        "            par2orph[id2syns[par_id]] = id2syns[child_id]\n",
        "            orph2par[id2syns[child_id]] = id2syns[par_id]"
      ],
      "metadata": {
        "id": "DGWVX-WL74KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "for word in fasttext.key_to_index:\n",
        "    if \".n.\" not in word:\n",
        "        cur_index = fasttext.key_to_index[word]\n",
        "        tensor_ = torch.tensor([[cur_index]*(len(G.nodes)), [i for i in range(0, len(G.nodes))]])\n",
        "        results = model.decode(z, tensor_)\n",
        "        top10 = list(reversed(sorted([(index_to_key[i], round(float(score.cpu().detach().float()), 4)) for i, score in enumerate(results)], key=lambda x: x[1])))[:10]\n",
        "        print(orph2par[word], \":\", top10)\n",
        "        print(\"=\"*10)\n",
        "        c += 1\n",
        "        if c == 20:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4jW1K0d3Fq1",
        "outputId": "dda824a2-b03d-4ff8-bc13-2a0e0b0f8815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "course.n.04 : [('action.n.06', 0.9923), ('action.n.09', 0.9923), ('act.n.05', 0.9918), ('pump_action.n.01', 0.9906), ('job_action.n.01', 0.9891), ('piano_action.n.01', 0.989), ('movement.n.10', 0.9875), ('police_action.n.01', 0.9871), ('gunlock.n.01', 0.9859), ('course.n.04', 0.9859)]\n",
            "==========\n",
            "recovery.n.03 : [('rescue.n.01', 0.9973), ('ORPHAN_100000001', 0.9966), ('recovery.n.03', 0.992), ('purge.n.02', 0.9875), ('purification.n.01', 0.9859), ('sanitation.n.02', 0.9858), ('sterilization.n.02', 0.9858), ('due.n.02', 0.9856), ('scrub.n.02', 0.9856), ('housecleaning.n.02', 0.9853)]\n",
            "==========\n",
            "disappearance.n.01 : [('three-d.n.01', 0.9825), ('due.n.02', 0.9377), ('ORPHAN_100000002', 0.919), ('inflow.n.01', 0.9162), ('ripple.n.02', 0.9108), ('special_effect.n.01', 0.9039), ('iris.n.02', 0.9015), ('financial_loss.n.01', 0.9011), ('bilocation.n.01', 0.8988), ('sound_effect.n.01', 0.8963)]\n",
            "==========\n",
            "hit.n.03 : [('base_hit.n.01', 0.9774), ('act.n.05', 0.9618), ('blow.n.02', 0.9537), ('hit.n.01', 0.9475), ('ORPHAN_100000003', 0.9458), ('collision.n.01', 0.9454), ('slam.n.03', 0.9435), ('swimming_kick.n.01', 0.9306), ('squib.n.01', 0.9283), ('hit.n.03', 0.9283)]\n",
            "==========\n",
            "breach.n.01 : [('breach_of_warranty.n.01', 0.9967), ('breach_of_promise.n.01', 0.9964), ('breach_of_trust.n.01', 0.9961), ('breach_of_the_covenant_of_warranty.n.01', 0.9959), ('anticipatory_breach.n.01', 0.9951), ('partial_breach.n.01', 0.9942), ('ORPHAN_100000004', 0.9936), ('instrument_of_torture.n.01', 0.9899), ('breach_of_contract.n.01', 0.9875), ('due.n.02', 0.9852)]\n",
            "==========\n",
            "buying.n.01 : [('ORPHAN_100000005', 0.9998), ('shopping.n.01', 0.9997), ('buying.n.01', 0.9995), ('prior.n.01', 0.9964), ('receipt.n.02', 0.9958), ('stub.n.03', 0.9949), ('choice.n.01', 0.9936), ('process_printing.n.01', 0.9934), ('resale.n.01', 0.9933), ('forging.n.01', 0.9933)]\n",
            "==========\n",
            "restitution.n.03 : [('ORPHAN_100000006', 0.9844), ('pocket_veto.n.01', 0.9814), ('payee.n.01', 0.9662), ('bylaw.n.01', 0.9634), ('brier.n.02', 0.9541), ('uvula.n.01', 0.9517), ('appropriation_bill.n.01', 0.9508), ('ORPHAN_100000018', 0.9457), ('sliding_seat.n.01', 0.943), ('bill_of_attainder.n.01', 0.9419)]\n",
            "==========\n",
            "abandonment.n.03 : [('abandonment.n.03', 0.9847), ('sewage_disposal.n.01', 0.9807), ('act.n.05', 0.9739), ('giving.n.03', 0.9732), ('mine_disposal.n.01', 0.9722), ('ORPHAN_100000007', 0.9695), ('lending.n.01', 0.9688), ('quitclaim.n.01', 0.9673), ('comb-out.n.02', 0.9656), ('cession.n.01', 0.9651)]\n",
            "==========\n",
            "mine_disposal.n.01 : [('mine_disposal.n.01', 0.9524), ('sewage_disposal.n.01', 0.9459), ('abandonment.n.03', 0.9321), ('lending.n.01', 0.9262), ('comb-out.n.02', 0.9162), ('hand_drill.n.01', 0.9158), ('giving.n.03', 0.9154), ('safe.n.01', 0.9136), ('disposal.n.03', 0.9055), ('quitclaim.n.01', 0.8931)]\n",
            "==========\n",
            "expiation.n.02 : [('exception.n.01', 0.9862), ('omission.n.04', 0.9791), ('ORPHAN_100000009', 0.9786), ('reparation.n.01', 0.973), ('atonement.n.01', 0.9702), ('avarice.n.01', 0.9697), ('lust.n.02', 0.9668), ('honorarium.n.01', 0.9666), ('punitive_damages.n.01', 0.9662), ('pride.n.05', 0.966)]\n",
            "==========\n",
            "rendition.n.04 : [('backspin.n.01', 0.9235), ('nudge.n.01', 0.9185), ('turning.n.04', 0.9159), ('whirl.n.01', 0.9102), ('ORPHAN_100000010', 0.9094), ('wave.n.03', 0.9085), ('orb_web.n.01', 0.908), ('bending.n.01', 0.9061), ('wave.n.02', 0.9038), ('yaw.n.01', 0.9033)]\n",
            "==========\n",
            "depression.n.10 : [('ORPHAN_100000011', 0.9878), ('touch.n.09', 0.9825), ('sketch_map.n.01', 0.9724), ('road_map.n.02', 0.9701), ('weather_map.n.01', 0.9682), ('tap.n.05', 0.9682), ('see.n.01', 0.9673), ('swipe.n.01', 0.967), ('moot.n.01', 0.9653), ('first-place_finish.n.01', 0.9631)]\n",
            "==========\n",
            "blink.n.01 : [('eye.n.03', 0.9887), ('eye.n.05', 0.9887), ('touch.n.09', 0.9766), ('saccade.n.01', 0.9649), ('eye_contact.n.01', 0.9643), ('ORPHAN_100000012', 0.9402), ('wailing.n.01', 0.9317), ('gesture.n.03', 0.93), ('eye_movement.n.01', 0.9293), ('blink.n.01', 0.9226)]\n",
            "==========\n",
            "shooting.n.01 : [('shot.n.12', 0.9969), ('set_shot.n.01', 0.9916), ('pivot_shot.n.01', 0.9915), ('dunk.n.01', 0.9876), ('bank_shot.n.01', 0.9865), ('jumper.n.08', 0.9857), ('headshot.n.02', 0.985), ('cheap_shot.n.02', 0.9847), ('shoot.n.02', 0.9839), ('discharge.n.09', 0.9831)]\n",
            "==========\n",
            "hit.n.02 : [('act.n.05', 0.988), ('touch.n.09', 0.9823), ('base_hit.n.01', 0.982), ('hit.n.02', 0.9778), ('fly.n.04', 0.9726), ('grounder.n.01', 0.9694), ('crash.n.04', 0.9667), ('ORPHAN_100000014', 0.9667), ('power_play.n.03', 0.9666), ('xy.n.01', 0.9657)]\n",
            "==========\n",
            "free_kick.n.01 : [('swimming_kick.n.01', 0.9696), ('tab.n.04', 0.9593), ('kick.n.05', 0.9539), ('kick.n.06', 0.9473), ('place_kick.n.01', 0.9332), ('pop_quiz.n.01', 0.9223), ('financial_gain.n.01', 0.9156), ('wild_ass.n.01', 0.9154), ('dolphin_kick.n.01', 0.915), ('scissors_kick.n.01', 0.9129)]\n",
            "==========\n",
            "assignment.n.03 : [('ORPHAN_100000016', 0.9977), ('abstraction.n.06', 0.9957), ('assignment.n.03', 0.9939), ('thing.n.08', 0.9932), ('sorting.n.03', 0.9929), ('entity.n.01', 0.99), ('phrasing.n.01', 0.9886), ('job.n.11', 0.9884), ('job.n.04', 0.9884), ('job.n.03', 0.9884)]\n",
            "==========\n",
            "road.n.02 : [('towpath.n.01', 0.9979), ('walk.n.05', 0.9966), ('path.n.02', 0.9945), ('ORPHAN_100000017', 0.9932), ('side_road.n.01', 0.9908), ('direction.n.01', 0.9897), ('track.n.10', 0.9886), ('speedway.n.01', 0.9878), ('road.n.02', 0.9874), ('trade_route.n.02', 0.9869)]\n",
            "==========\n",
            "vote.n.02 : [('ORPHAN_100000018', 1.0), ('vote.n.02', 0.9999), ('pocket_veto.n.01', 0.9994), ('veto.n.01', 0.999), ('vote.n.05', 0.9977), ('casting_vote.n.01', 0.9975), ('block_vote.n.01', 0.9971), ('ORPHAN_100000385', 0.9967), ('ban.n.03', 0.9965), ('secret_ballot.n.01', 0.9958)]\n",
            "==========\n",
            "economy.n.04 : [('ORPHAN_100000019', 0.9936), ('economy.n.04', 0.9872), ('rapid.n.01', 0.9731), ('entail.n.02', 0.9724), ('ORPHAN_100000005', 0.9694), ('life.n.08', 0.9692), ('shopping.n.01', 0.9674), ('foray.n.02', 0.9632), ('buying.n.01', 0.9555), ('net.n.04', 0.9554)]\n",
            "==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Networks v2 (GATv2)\n",
        "\n",
        "This is a PyTorch implementation of the GATv2 operator from the paper How Attentive are Graph Attention Networks?.\n",
        "\n",
        "https://nn.labml.ai/graphs/gatv2/index.html"
      ],
      "metadata": {
        "id": "feq6fTEdTeMe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgbmlZqBIJEN"
      },
      "source": [
        "## GraphBERT\n",
        "\n",
        "https://github.com/jwzhanggy/Graph-Bert\n",
        "\n",
        "Yet another model for embedding generation is GraphBert. Instead of feeding large input graph, we train GRAPH-BERT with sampled subgraphs within their local contexts. The input vector embeddings to be fed to the graphtransformer model actually cover four parts: (1) raw feature vector embedding, (2) Weisfeiler-Lehman absolute role embedding, (3) intimacy based relative positional embedding, and (4) hop based relative distance embedding, respectively.\n",
        "\n",
        "GRAPH-BERT is trained with the node attribute reconstruction and structure recovery tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0z3trCa2BC"
      },
      "source": [
        "![](https://github.com/jwzhanggy/Graph-Bert/raw/master/result/screenshot/model.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOqFVHLa2BD"
      },
      "source": [
        "## Subgraph Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5htpR5J7a2BD"
      },
      "source": [
        "![](https://i.ibb.co/5cbjJZ6/photo-2021-12-07-16-41-32.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDsdGrffa2BD"
      },
      "source": [
        "## Positional embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOny-pLQa2BE"
      },
      "source": [
        "### Weisfeiler-Lehman Absolute Role Embedding\n",
        "\n",
        "![](https://i.ibb.co/bgT7gqb/wl.png)\n",
        "\n",
        "### Intimacy based Relative Positional Embedding\n",
        "\n",
        "![](https://i.ibb.co/34FvCf0/photo-2021-12-07-16-52-30.jpg)\n",
        "\n",
        "### Hop based Relative Distance Embedding\n",
        "![](https://i.ibb.co/tCzRcfK/hops-drawio.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AomlgEcXa2BE"
      },
      "source": [
        "Actually, you are simply expected to run two scripts: `script_1_preprocess.py` and `script_2_pre_train.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DXA9H5la2BF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736a899d-efd0-4285-8b44-ff177aa6019d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Graph-Bert'...\n",
            "remote: Enumerating objects: 450, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 450 (delta 106), reused 79 (delta 78), pack-reused 314\u001b[K\n",
            "Receiving objects: 100% (450/450), 2.23 MiB | 12.33 MiB/s, done.\n",
            "Resolving deltas: 100% (232/232), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jwzhanggy/Graph-Bert.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueo45TqXa2BF",
        "outputId": "32c986b4-f44f-4855-d271-c5f1614783c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Graph-Bert\n",
            "************ Start ************\n",
            "WL, dataset: cora\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 1\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 2\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 3\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 4\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 5\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 6\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 7\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 8\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 9\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: cora, k: 10\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 1\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 2\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 3\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 4\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 5\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 6\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 7\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 8\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 9\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: cora, k: 10\n",
            "Loading cora dataset...\n",
            "************ Finish ************\n"
          ]
        }
      ],
      "source": [
        "%cd Graph-Bert\n",
        "!python3 script_1_preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8IhWJJ4QcWs",
        "outputId": "6a399892-bb4d-4bc9-e8b9-eb6c9ec0cf4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 script_2_pre_train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1p4Er7hQSNo",
        "outputId": "fb397eab-1d04-4c92-e03b-4c62576c2141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Graph-Bert/script_2_pre_train.py\", line 5, in <module>\n",
            "    from code.MethodBertComp import GraphBertConfig\n",
            "  File \"/content/Graph-Bert/code/MethodBertComp.py\", line 11, in <module>\n",
            "    from transformers.modeling_bert import BertPredictionHeadTransform, BertAttention, BertIntermediate, BertOutput\n",
            "ModuleNotFoundError: No module named 'transformers.modeling_bert'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE-7RzJ1a2BK"
      },
      "source": [
        "## View and evaluate results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1IAfd9tRgtVtdosM5vuDdxh-VSBFp3mzI\n",
        "!gdown 1LItbxEcchOfU4TrlLBZjQweC8jpQ3b3Q\n",
        "!gdown 1VLLLyu9YyLX3uCojiTm_VLtgK2gKCCfW\n",
        "!gdown 1h5sSbFeCJbouH96fKIZDF2xugNiKf3La"
      ],
      "metadata": {
        "id": "zvWT_UAaSW7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9331aa-5fb0-4d1b-fdb0-5b0c77c488f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IAfd9tRgtVtdosM5vuDdxh-VSBFp3mzI\n",
            "To: /content/Graph-Bert/MethodGraphBertGraphRecovery_model_test_embeddings.txt\n",
            "100% 258k/258k [00:00<00:00, 133MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LItbxEcchOfU4TrlLBZjQweC8jpQ3b3Q\n",
            "To: /content/Graph-Bert/MethodGraphBertGraphRecovery_model_train_embeddings_.txt\n",
            "100% 196M/196M [00:05<00:00, 34.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VLLLyu9YyLX3uCojiTm_VLtgK2gKCCfW\n",
            "To: /content/Graph-Bert/MethodGraphBertNodeConstruct_model_train_embeddings_.txt\n",
            "100% 288M/288M [00:09<00:00, 31.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h5sSbFeCJbouH96fKIZDF2xugNiKf3La\n",
            "To: /content/Graph-Bert/MethodGraphBertNodeConstruct_model_test_embeddings.txt\n",
            "100% 362k/362k [00:00<00:00, 132MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FyqIy_MowB_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "uLCgr2ibUZL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW9Z8hLca2BL"
      },
      "outputs": [],
      "source": [
        "graphBertNode_train = KeyedVectors.load_word2vec_format(\"MethodGraphBertNodeConstruct_model_train_embeddings_.txt\")\n",
        "graphBertNode_test = KeyedVectors.load_word2vec_format(\"MethodGraphBertNodeConstruct_model_test_embeddings.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdnqyA7ia2BL",
        "outputId": "6cd82420-242f-484b-e1b4-b98213a12565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hound.n.01', 0.8980603814125061),\n",
              " ('working_dog.n.01', 0.8848254680633545),\n",
              " ('dandy.n.01', 0.8751208782196045),\n",
              " ('old_man.n.01', 0.8639864325523376),\n",
              " ('professional.n.01', 0.8605043888092041),\n",
              " ('gravida.n.02', 0.849956750869751),\n",
              " ('child.n.02', 0.8499069809913635),\n",
              " ('spaniel.n.01', 0.8490362167358398),\n",
              " ('subordinate.n.01', 0.8471304178237915),\n",
              " ('parent.n.01', 0.8452426195144653)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "graphBertNode_train.similar_by_word(\"dog.n.01\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext.similar_by_word(\"dog.n.01\")"
      ],
      "metadata": {
        "id": "wyTvIjVQwy0B",
        "outputId": "b6b764f0-ed3c-495d-f062-8b23d3a3edfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dog.n.03', 0.9556826949119568),\n",
              " ('seizure-alert_dog.n.01', 0.9394533634185791),\n",
              " ('working_dog.n.01', 0.9265448451042175),\n",
              " ('dog_breeding.n.01', 0.9224575757980347),\n",
              " ('hunting_dog.n.01', 0.9179456233978271),\n",
              " ('dog_biscuit.n.01', 0.9155749678611755),\n",
              " ('ORPHAN_100000208', 0.9101989269256592),\n",
              " ('dog_catcher.n.01', 0.9075720906257629),\n",
              " ('raccoon_dog.n.01', 0.9067509770393372),\n",
              " ('dalmatian.n.02', 0.9053666591644287)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset(\"dog.n.01\").hypernyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAEmE4jXwJR6",
        "outputId": "f7301318-193f-4ab9-dfbc-d3580ffd1bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenHGNN library\n",
        "\n",
        "https://github.com/BUPT-GAMMA/OpenHGNN"
      ],
      "metadata": {
        "id": "SZbGESB3VSYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMEBnLKLVTyp",
        "outputId": "f7e8f7e6-297a-4140-b717-c0becf2d8127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dgl -f https://data.dgl.ai/wheels/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDdKxXeQVXjW",
        "outputId": "4f8e287e-1174-42ff-8d5f-071cf917417d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl\n",
            "  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openhgnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjzYAtcbVaKA",
        "outputId": "9aa4462a-58e2-46a3-9a45-285da93387b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openhgnn\n",
            "  Downloading openhgnn-0.4.0.tar.gz (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.7/230.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from openhgnn) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from openhgnn) (1.5.3)\n",
            "Collecting ogb>=1.1.0 (from openhgnn)\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from openhgnn) (2.0.1+cu118)\n",
            "Collecting optuna (from openhgnn)\n",
            "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama (from openhgnn)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.1.0->openhgnn) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.1.0->openhgnn) (1.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.1.0->openhgnn) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.1.0->openhgnn) (1.26.15)\n",
            "Collecting outdated>=0.2.0 (from ogb>=1.1.0->openhgnn)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->openhgnn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->openhgnn) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->openhgnn) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->openhgnn) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->openhgnn) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->openhgnn) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->openhgnn) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->openhgnn) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->openhgnn) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->openhgnn) (16.0.5)\n",
            "Collecting alembic>=1.5.0 (from optuna->openhgnn)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna->openhgnn)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna->openhgnn)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna->openhgnn) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->openhgnn) (2.0.10)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna->openhgnn) (6.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->openhgnn)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.1.0->openhgnn) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb>=1.1.0->openhgnn)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.1.0->openhgnn) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb>=1.1.0->openhgnn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb>=1.1.0->openhgnn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb>=1.1.0->openhgnn) (3.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->openhgnn) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->openhgnn) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->openhgnn) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.1.0->openhgnn) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.1.0->openhgnn) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.1.0->openhgnn) (3.4)\n",
            "Building wheels for collected packages: openhgnn, littleutils\n",
            "  Building wheel for openhgnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openhgnn: filename=openhgnn-0.4.0-py3-none-any.whl size=297048 sha256=b675b921fa182061a58b8c9c363a1a357babbd1018cbde4c6859148bb4d605f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/23/6b/e05cfeeb232b3b82a75ccf9a49d9884602e7dc8dc375c85e84\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=b67e8168dc8f9e0e8c2a0f2b2ea886aaadc42f41b2a30e4d49cc6beeb6a154c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built openhgnn littleutils\n",
            "Installing collected packages: littleutils, Mako, colorlog, colorama, cmaes, outdated, alembic, optuna, ogb, openhgnn\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorama-0.4.6 colorlog-6.7.0 littleutils-0.2.2 ogb-1.3.6 openhgnn-0.4.0 optuna-3.1.1 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2BZSr40VsL_",
        "outputId": "018e68db-1f0e-44d5-f53a-1a0e4fe19d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BUPT-GAMMA/OpenHGNN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7v8yANvVyh-",
        "outputId": "9ad9bb27-5720-4f19-83f1-4f828dccc405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OpenHGNN'...\n",
            "remote: Enumerating objects: 6676, done.\u001b[K\n",
            "remote: Counting objects: 100% (505/505), done.\u001b[K\n",
            "remote: Compressing objects: 100% (212/212), done.\u001b[K\n",
            "remote: Total 6676 (delta 322), reused 438 (delta 289), pack-reused 6171\u001b[K\n",
            "Receiving objects: 100% (6676/6676), 15.26 MiB | 8.22 MiB/s, done.\n",
            "Resolving deltas: 100% (4710/4710), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from networkx.algorithms.centrality import edge_betweenness_centrality"
      ],
      "metadata": {
        "id": "6y84LVj_W1m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./OpenHGNN/main.py -m GTN -d imdb4GTN -t node_classification -g -1 --use_best_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14N_Z_UWVcbf",
        "outputId": "4b32a4b9-2450-4b80-ec98-12844ec41b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"//./OpenHGNN/main.py\", line 8, in <module>\n",
            "    from openhgnn.experiment import Experiment\n",
            "  File \"/OpenHGNN/openhgnn/__init__.py\", line 4, in <module>\n",
            "    from .trainerflow import *\n",
            "  File \"/OpenHGNN/openhgnn/trainerflow/__init__.py\", line 89, in <module>\n",
            "    from .slice_trainer import SLiCETrainer\n",
            "  File \"/OpenHGNN/openhgnn/trainerflow/slice_trainer.py\", line 11, in <module>\n",
            "    from networkx.algorithms.centrality.betweenness import edge_betweenness\n",
            "ImportError: cannot import name 'edge_betweenness' from 'networkx.algorithms.centrality.betweenness' (/usr/local/lib/python3.10/dist-packages/networkx/algorithms/centrality/betweenness.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nwbPHpUYrqHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}